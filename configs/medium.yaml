# medium config for 30 mins to 4 hours of data
data:
  data_dir: "training_dataset"
  sample_rate: 16000
  num_val_files: 6
  max_seq_len: null
  frame_duration: 0.02
  n_mels: 80

model:
  encoder_type: "whisper"
  whisper_model: "openai/whisper-base"
  wavlm_model: "microsoft/wavlm-base-plus"
  
  # unfreeze last 2 layers to adapt to speaker's mic/accent
  freeze_encoder: true 
  unfreeze_last_n_layers: 2 
  
  num_conformer_layers: 4 
  conformer_heads: 4
  conformer_ff_expansion: 4
  conformer_kernel_size: 31
  conformer_dropout: 0.15

  enable_dilated_conv: true
  dilated_conv_depth: 3
  dilated_conv_kernel: 3

  subframe_loss_weight: 5.0 

  lang_emb_dim: 64
  num_languages: 0

training:
  batch_size: 8
  num_workers: 4
  optimizer: "AdamW"
  
  learning_rate: 0.0001 
  weight_decay: 0.01
  
  max_epochs: 1000
  check_val_every_n_epoch: 2 # Validate every 2 epochs
  lr_decay_every_n_epochs: 2 # lr reduce every 2 epochs
  lr_decay_gamma: 0.9
  
  max_checkpoints: 3
  log_dir: "logs/standard"
  merged_phoneme_groups: []

augmentation:
  enable: true
  noise_std: 0.005
  prob: 0.5
  volume_range: [0.8, 1.2]

output:
  save_dir: "checkpoints_standard"
  
postprocess:
  median_filter: 1
  merge_segments: "right"