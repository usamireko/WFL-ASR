{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamireko/WFL-ASR/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structure Guide"
      ],
      "metadata": {
        "id": "x3RzmEiSgrGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip file\n",
        "___\n",
        "\n",
        "```\n",
        "your_zip.zip\n",
        "├── folder1\n",
        "│   ├── audio.wav\n",
        "│   └── audio.lab\n",
        "│   └── ...\n",
        "├── folder2\n",
        "│   ├── audio.wav\n",
        "│   └── audio.lab\n",
        "│   └── ...\n",
        "├── folder3\n",
        "│   ├── audio.wav\n",
        "│   └── audio.lab\n",
        "│   └── ...\n",
        "...\n",
        "```\n",
        "\n",
        "> Folder names will be used as language name. For language code, look at langs.txt you got after preprocessing\n",
        "\n",
        "- Both `.wav` and `.lab` filenames should match within each folder (example: `song.wav` and `song.lab`)\n",
        "___\n",
        "\n",
        "### Example\n",
        "\n",
        "```\n",
        "(multi-lang)\n",
        "data.zip\n",
        "├── JPN\n",
        "│   ├── sample_001.wav\n",
        "│   └── sample_001.lab\n",
        "│   └── ...\n",
        "├── ENG\n",
        "│   ├── sample_001.wav\n",
        "│   └── sample_001.lab\n",
        "│   └── ...\n",
        "```\n",
        "\n",
        "```\n",
        "(single-lang)\n",
        "data.zip\n",
        "├── JPN\n",
        "│   ├── sample_001.wav\n",
        "│   └── sample_001.lab\n",
        "│   └── ...\n",
        "```\n",
        "\n",
        "Please try to avoid bad labels like trash phonemes or labels, there's a risk of them getting predicted!\n"
      ],
      "metadata": {
        "id": "cKyvyRZsfMXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "RY3cKNDTgyXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB099J18COAw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Setup\n",
        "#```pip install torch torchaudio soundfile transformers torchcrf matplotlib tqdm pyyaml tensorboard```\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"60\"\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/MLo7Ghinsan/WFL-ASR.git\n",
        "!pip install pytorch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQEp3nEixH9D",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Extract data\n",
        "\n",
        "\n",
        "\n",
        "%cd /content\n",
        "\n",
        "\n",
        "!apt-get install -y p7zip-full\n",
        "!mkdir /content/training_dataset\n",
        "\n",
        "data_zip = \"\" #@param {type:\"string\"}\n",
        "#!7z x /content/drive/MyDrive/WFL_Training_Kit/long_data.zip -o/content/training_dataset\n",
        "#!7z x /content/drive/MyDrive/WFL_Training_Kit/Neiro_only.zip -o/content/training_dataset\n",
        "#!7z x /content/drive/MyDrive/WFL_Finetuning_data.zip -o/content/training_dataset\n",
        "!7z x {data_zip} -o/content/training_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Edit Config\n",
        "# copied from @HAI-D, im too lazy to make this, ty <333\n",
        "\n",
        "data_dir = \"/content/training_dataset\"\n",
        "#@markdown # Model\n",
        "#@markdown ___\n",
        "encoder_type = \"whisper\" #@param [\"whisper\", \"wavlm\"]\n",
        "whisper_model = \"small\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
        "whisper_model_path = \"openai/whisper-\" + whisper_model\n",
        "wavlm_model = \"base\" #@param [\"base\", \"base-sd\", \"base-sv\", \"base-plus\", \"base-plus-sd\", \"base-plus-sv\", \"wavlm-large\"]\n",
        "wavlm_model_path = \"microsoft/wavlm-\" + wavlm_model\n",
        "#@markdown <font size=\"-1.5\"> False to finetune the encoder, True to not finetune\n",
        "freeze_encoder = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown # Training\n",
        "#@markdown ___\n",
        "#@markdown <font size=\"-1.5\"> Batch size finally working!\n",
        "#@markdown <font size=\"-1.5\"> Prodigy optimizer set as default, lr requires to be 1.0, change if using other optimizer manually\n",
        "batch_size = 16 #@param {type: \"integer\"}\n",
        "num_workers = 4 #@param {type: \"integer\"}\n",
        "learning_rate = 1 #@param {type: \"number\"}\n",
        "#@markdown <font size=\"-1.5\"> learning rate decay gamma every val_check_interval (set to 1 for Prodigy)\n",
        "lr_decay_gamma = 1 #@param {type: \"number\"}\n",
        "#@markdown <font size=\"-1.5\"> prevent `overconfident` labels\n",
        "label_smoothing = 0.1 #@param {type: \"number\"}\n",
        "#markdown <font size=\"-1.5\"> path to logs folder for tensorboard\n",
        "#log_dir = \"/content/drive/MyDrive/WFL/Model/logs\" #@param {type: \"string\"}\n",
        "#markdown <font size=\"-1.5\"> path to checkpoints folder\n",
        "#@markdown # Finetuning\n",
        "#@markdown ___\n",
        "#@markdown <font size=\"-1.5\"> enabling WLF finetuning\n",
        "finetune_enbale = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"-1.5\"> path to finetune model\n",
        "finetune_model_path = \"\" #@param {type: \"string\"}\n",
        "#@markdown # Saving\n",
        "#@markdown ___\n",
        "#@markdown <font size=\"-1.5\"> path to save folder\n",
        "save_dir = \"\" #@param {type: \"string\"}\n",
        "#@markdown <font size=\"-1.5\"> training stop point\n",
        "max_steps = 100000 # @param {\"type\":\"slider\",\"min\":1000,\"max\":1500000,\"step\":1000}\n",
        "#@markdown <font size=\"-1.5\"> validation/saving interval\n",
        "val_check_interval = 500 # @param {\"type\":\"slider\",\"min\":100,\"max\":10000,\"step\":100}\n",
        "\n",
        "if not save_dir:\n",
        "    raise ValueError(\"save_dir is not set, please set a saving directory\")\n",
        "import re\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "log_dir = os.path.join(save_dir, \"logs\")\n",
        "\n",
        "with open(\"/content/WFL-ASR/config.yaml\", \"r\") as config:\n",
        "    wfl_config = yaml.safe_load(config)\n",
        "#data\n",
        "wfl_config[\"data\"][\"data_dir\"] = data_dir\n",
        "#model\n",
        "wfl_config[\"model\"][\"encoder_type\"] = encoder_type\n",
        "wfl_config[\"model\"][\"whisper_model\"] = whisper_model_path\n",
        "wfl_config[\"model\"][\"wavlm_model\"] = wavlm_model_path\n",
        "wfl_config[\"model\"][\"freeze_encoder\"] = freeze_encoder\n",
        "#training\n",
        "wfl_config[\"training\"][\"batch_size\"] = batch_size\n",
        "wfl_config[\"training\"][\"num_workers\"] = num_workers\n",
        "wfl_config[\"training\"][\"learning_rate\"] = learning_rate\n",
        "wfl_config[\"training\"][\"lr_decay_gamma\"] = lr_decay_gamma\n",
        "wfl_config[\"training\"][\"label_smoothing\"] = label_smoothing\n",
        "wfl_config[\"training\"][\"max_steps\"] = max_steps\n",
        "wfl_config[\"training\"][\"val_check_interval\"] = val_check_interval\n",
        "wfl_config[\"training\"][\"log_dir\"] = log_dir\n",
        "#finetuning\n",
        "wfl_config[\"finetuning\"][\"enable\"] = finetune_enbale\n",
        "wfl_config[\"finetuning\"][\"model_path\"] = finetune_model_path\n",
        "#output\n",
        "wfl_config[\"output\"][\"save_dir\"] = save_dir\n",
        "\n",
        "with open(\"/content/WFL-ASR/config.yaml\", \"w\") as config:\n",
        "    yaml.dump(wfl_config, config)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SAeIDRGjhFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-ihGfzM1ppg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%cd /content/WFL-ASR\n",
        "#@title # Preprocess\n",
        "!python preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RImPGfPW3mWo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Training\n",
        "#@markdown Input the config you got after preprocessing\n",
        "config_path = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "with open(config_path, \"r\") as config:\n",
        "    wfl_config = yaml.safe_load(config)\n",
        "\n",
        "log_dir = wfl_config[\"training\"][\"log_dir\"]\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}\n",
        "%cd /content/WFL-ASR\n",
        "!python train.py {config_path}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}