# small config for below 30 mins of data
data:
  data_dir: "training_dataset"
  sample_rate: 16000
  num_val_files: 4
  max_seq_len: null
  frame_duration: 0.02
  n_mels: 80

model:
  encoder_type: "whisper"
  whisper_model: "openai/whisper-base"
  wavlm_model: "microsoft/wavlm-base-plus"
  
  # totally frozen encoder. below 30 mins is not enough to finetune whisper
  freeze_encoder: true 
  unfreeze_last_n_layers: 0 
  
  num_conformer_layers: 2 
  conformer_heads: 2
  conformer_ff_expansion: 2
  conformer_kernel_size: 15
  conformer_dropout: 0.3

  enable_dilated_conv: true
  dilated_conv_depth: 2
  dilated_conv_kernel: 3

  subframe_loss_weight: 5.0 

  lang_emb_dim: 64
  num_languages: 0

training:
  batch_size: 4
  num_workers: 2
  optimizer: "AdamW"
  
  learning_rate: 0.0001 
  weight_decay: 0.01
  
  max_epochs: 1000 
  check_val_every_n_epoch: 1 # Validate every epoch
  lr_decay_every_n_epochs: 1 # lr reduce every epoch
  lr_decay_gamma: 0.9
  
  max_checkpoints: 3
  log_dir: "logs/micro"
  merged_phoneme_groups: []

augmentation:
  enable: true
  noise_std: 0.01
  prob: 0.8
  volume_range: [0.5, 1.5]

output:
  save_dir: "checkpoints_micro"
  
postprocess:
  median_filter: 1
  merge_segments: "right"